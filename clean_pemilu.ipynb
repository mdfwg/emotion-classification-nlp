{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ebUIan1kWjuC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "import re\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cbpZout6E5rg"
      },
      "outputs": [],
      "source": [
        "df_emosi=pd.read_csv(r'src\\Twitter_Emotion_Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-2R82OCCzTMQ"
      },
      "outputs": [],
      "source": [
        "df_pemilu=pd.read_csv(r'src\\pemilu-2024.csv', delimiter=';', on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baIkTC_gIQlP"
      },
      "source": [
        "### Missval Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GebdnPDcIU5x",
        "outputId": "9ec5fb91-0d22-4fe0-d953-a275b41ba2c7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_at</th>\n",
              "      <th>id_str</th>\n",
              "      <th>full_text</th>\n",
              "      <th>quote_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>lang</th>\n",
              "      <th>user_id_str</th>\n",
              "      <th>conversation_id_str</th>\n",
              "      <th>username</th>\n",
              "      <th>tweet_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tue Feb 13 23:57:50 +0000 2024</td>\n",
              "      <td>1757554706011009374</td>\n",
              "      <td>@belialrising Keturunan indon? Tgk je la indon...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>in</td>\n",
              "      <td>1623528833793265665</td>\n",
              "      <td>1757325132274819532</td>\n",
              "      <td>reform_roar</td>\n",
              "      <td>https://twitter.com/reform_roar/status/1757554...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tue Feb 13 23:55:54 +0000 2024</td>\n",
              "      <td>1757554220851662851</td>\n",
              "      <td>Calon wakil presiden (cawapres) nomor urut 1, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>in</td>\n",
              "      <td>280794125</td>\n",
              "      <td>1757554220851662851</td>\n",
              "      <td>kalimaya_malang</td>\n",
              "      <td>https://twitter.com/kalimaya_malang/status/175...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tue Feb 13 23:55:48 +0000 2024</td>\n",
              "      <td>1757554194138157113</td>\n",
              "      <td>#02JariUngu sore sudah ketauan hasil pemilu pr...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>in</td>\n",
              "      <td>1402500059929800708</td>\n",
              "      <td>1757554194138157113</td>\n",
              "      <td>lalalalisae_m</td>\n",
              "      <td>https://twitter.com/lalalalisae_m/status/17575...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tue Feb 13 23:52:42 +0000 2024</td>\n",
              "      <td>1757553415172059371</td>\n",
              "      <td>siapapun bapak-bapak yang terpilih menjadi pre...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>in</td>\n",
              "      <td>1410553792865607686</td>\n",
              "      <td>1757553415172059371</td>\n",
              "      <td>caastagfirullah</td>\n",
              "      <td>https://twitter.com/caastagfirullah/status/175...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tue Feb 13 23:50:05 +0000 2024</td>\n",
              "      <td>1757552758109163895</td>\n",
              "      <td>@HisyamMochtar Pada tahun 20(24-Umat), rakyat ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>in</td>\n",
              "      <td>164950723</td>\n",
              "      <td>1757368895965729041</td>\n",
              "      <td>eindro76</td>\n",
              "      <td>https://twitter.com/eindro76/status/1757552758...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       created_at               id_str  \\\n",
              "0  Tue Feb 13 23:57:50 +0000 2024  1757554706011009374   \n",
              "1  Tue Feb 13 23:55:54 +0000 2024  1757554220851662851   \n",
              "2  Tue Feb 13 23:55:48 +0000 2024  1757554194138157113   \n",
              "3  Tue Feb 13 23:52:42 +0000 2024  1757553415172059371   \n",
              "4  Tue Feb 13 23:50:05 +0000 2024  1757552758109163895   \n",
              "\n",
              "                                           full_text  quote_count  \\\n",
              "0  @belialrising Keturunan indon? Tgk je la indon...            0   \n",
              "1  Calon wakil presiden (cawapres) nomor urut 1, ...            0   \n",
              "2  #02JariUngu sore sudah ketauan hasil pemilu pr...            0   \n",
              "3  siapapun bapak-bapak yang terpilih menjadi pre...            0   \n",
              "4  @HisyamMochtar Pada tahun 20(24-Umat), rakyat ...            0   \n",
              "\n",
              "   reply_count  retweet_count  favorite_count lang          user_id_str  \\\n",
              "0            1              1               1   in  1623528833793265665   \n",
              "1            0              0               1   in            280794125   \n",
              "2            0              0               1   in  1402500059929800708   \n",
              "3            0              0               0   in  1410553792865607686   \n",
              "4            0              0               1   in            164950723   \n",
              "\n",
              "   conversation_id_str         username  \\\n",
              "0  1757325132274819532      reform_roar   \n",
              "1  1757554220851662851  kalimaya_malang   \n",
              "2  1757554194138157113    lalalalisae_m   \n",
              "3  1757553415172059371  caastagfirullah   \n",
              "4  1757368895965729041         eindro76   \n",
              "\n",
              "                                           tweet_url  \n",
              "0  https://twitter.com/reform_roar/status/1757554...  \n",
              "1  https://twitter.com/kalimaya_malang/status/175...  \n",
              "2  https://twitter.com/lalalalisae_m/status/17575...  \n",
              "3  https://twitter.com/caastagfirullah/status/175...  \n",
              "4  https://twitter.com/eindro76/status/1757552758...  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_pemilu.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfO2J75Z0Yjb"
      },
      "source": [
        "### Masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7ICp5M80bpm",
        "outputId": "313d0723-cc80-42a5-be87-189ad474cb75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total kata yang dimask dalam dataset: 0\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk menghitung kata yang dimask dalam sebuah teks\n",
        "def count_masked_words(text):\n",
        "    if isinstance(text, str):\n",
        "        pattern = r'\\[USERNAME\\]|\\[USER\\]'  # Pola regex untuk mencari [USERNAME] atau [USER]\n",
        "        matches = re.findall(pattern, text)\n",
        "        return len(matches)\n",
        "    else:\n",
        "        return 0  # Jika nilai tidak bertipe string, kembalikan 0\n",
        "\n",
        "# Menerapkan fungsi ke kolom teks dalam DataFrame\n",
        "df_pemilu['masked_word_count'] = df_pemilu['full_text'].apply(count_masked_words)\n",
        "\n",
        "# Jumlah kata yang dimask dalam keseluruhan dataset\n",
        "total_masked_words = df_pemilu['masked_word_count'].sum()\n",
        "\n",
        "print(\"Total kata yang dimask dalam dataset:\", total_masked_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Wrfg6wLTJ_pA"
      },
      "outputs": [],
      "source": [
        "# Fungsi untuk menemukan dan menghitung pola yang diawali dengan tanda kurung siku dalam sebuah teks\n",
        "def find_and_count_patterns(text):\n",
        "    if isinstance(text, str):\n",
        "        pattern = r'\\[([^]]+)\\]'  # Pola regex untuk mencari semua pola yang diawali dengan tanda kurung siku\n",
        "        matches = re.findall(pattern, text)\n",
        "\n",
        "        # Menghitung jumlah kemunculan setiap pola\n",
        "        pattern_counts = {}\n",
        "        for match in matches:\n",
        "            if match in pattern_counts:\n",
        "                pattern_counts[match] += 1\n",
        "            else:\n",
        "                pattern_counts[match] = 1\n",
        "\n",
        "        return pattern_counts\n",
        "    else:\n",
        "        return {}  # Jika nilai tidak bertipe string, kembalikan dictionary kosong\n",
        "\n",
        "# Menerapkan fungsi ke kolom teks dalam DataFrame\n",
        "df_pemilu['pattern_counts'] = df_pemilu['full_text'].apply(find_and_count_patterns)\n",
        "\n",
        "# Menggabungkan hasil dari semua full_text menjadi satu dictionary\n",
        "all_patterns_counts = {}\n",
        "for pattern_count in df_pemilu['pattern_counts']:\n",
        "    for pattern, count in pattern_count.items():\n",
        "        if pattern in all_patterns_counts:\n",
        "            all_patterns_counts[pattern] += count\n",
        "        else:\n",
        "            all_patterns_counts[pattern] = count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNNmLuI8KM34",
        "outputId": "ee0aeafe-e883-4cb5-aced-bda63ff1c63d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SIARAN PERS 2\n",
            "3 1\n",
            " OPEN FOLLBACK  1\n",
            "OPINI 1\n",
            "Hari Pemilihan Suara 1\n",
            "NEWS EXPLAINED 1\n",
            "IKLAN LAYANAN MASYARAKAT 1\n"
          ]
        }
      ],
      "source": [
        "# Membuat barplot\n",
        "all_patterns_counts = sorted(all_patterns_counts.items(), key=lambda x:x[1], reverse=True)\n",
        "all_patterns_counts = dict(all_patterns_counts)\n",
        "for k, i in all_patterns_counts.items():\n",
        "  print(k, i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kzPQlh4lwoPq",
        "outputId": "c2d9b3d9-bd41-4eb7-8312-d3c7f977c4d8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_at</th>\n",
              "      <th>id_str</th>\n",
              "      <th>full_text</th>\n",
              "      <th>quote_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>lang</th>\n",
              "      <th>user_id_str</th>\n",
              "      <th>conversation_id_str</th>\n",
              "      <th>username</th>\n",
              "      <th>tweet_url</th>\n",
              "      <th>masked_word_count</th>\n",
              "      <th>pattern_counts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1494</th>\n",
              "      <td>Mon Feb 12 06:51:36 +0000 2024</td>\n",
              "      <td>1756934058377191563</td>\n",
              "      <td>Presiden Joko Widodo kembali menekankan kepada...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>in</td>\n",
              "      <td>1716092538833301504</td>\n",
              "      <td>1756934058377191563</td>\n",
              "      <td>Bebywoon</td>\n",
              "      <td>https://twitter.com/Bebywoon/status/1756934058...</td>\n",
              "      <td>0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1495</th>\n",
              "      <td>Mon Feb 12 06:50:48 +0000 2024</td>\n",
              "      <td>1756933857834856732</td>\n",
              "      <td>Denger denger Anies Muhaimin menang di Belanda...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>in</td>\n",
              "      <td>1694292049653366784</td>\n",
              "      <td>1756933857834856732</td>\n",
              "      <td>ohyaagitu</td>\n",
              "      <td>https://twitter.com/ohyaagitu/status/175693385...</td>\n",
              "      <td>0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>Mon Feb 12 06:49:05 +0000 2024</td>\n",
              "      <td>1756933425402151202</td>\n",
              "      <td>Tidak perlu menunggu presiden baru untuk memba...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>in</td>\n",
              "      <td>1755409910593179648</td>\n",
              "      <td>1756933425402151202</td>\n",
              "      <td>AzazilAzaz72537</td>\n",
              "      <td>https://twitter.com/AzazilAzaz72537/status/175...</td>\n",
              "      <td>0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>Mon Feb 12 06:48:35 +0000 2024</td>\n",
              "      <td>1756933299480785320</td>\n",
              "      <td>Presiden Joko Widodo kembali menekankan kepada...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>in</td>\n",
              "      <td>1356989898645929986</td>\n",
              "      <td>1756933267406606619</td>\n",
              "      <td>nobitatakesi</td>\n",
              "      <td>https://twitter.com/nobitatakesi/status/175693...</td>\n",
              "      <td>0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>Mon Feb 12 06:47:27 +0000 2024</td>\n",
              "      <td>1756933015492812906</td>\n",
              "      <td>Semoga 02 menang deh, biar makin pada sakit ha...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>in</td>\n",
              "      <td>1581628785815744512</td>\n",
              "      <td>1756933015492812906</td>\n",
              "      <td>Aditaahma</td>\n",
              "      <td>https://twitter.com/Aditaahma/status/175693301...</td>\n",
              "      <td>0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          created_at               id_str  \\\n",
              "1494  Mon Feb 12 06:51:36 +0000 2024  1756934058377191563   \n",
              "1495  Mon Feb 12 06:50:48 +0000 2024  1756933857834856732   \n",
              "1496  Mon Feb 12 06:49:05 +0000 2024  1756933425402151202   \n",
              "1497  Mon Feb 12 06:48:35 +0000 2024  1756933299480785320   \n",
              "1498  Mon Feb 12 06:47:27 +0000 2024  1756933015492812906   \n",
              "\n",
              "                                              full_text  quote_count  \\\n",
              "1494  Presiden Joko Widodo kembali menekankan kepada...            0   \n",
              "1495  Denger denger Anies Muhaimin menang di Belanda...            0   \n",
              "1496  Tidak perlu menunggu presiden baru untuk memba...            0   \n",
              "1497  Presiden Joko Widodo kembali menekankan kepada...            0   \n",
              "1498  Semoga 02 menang deh, biar makin pada sakit ha...            0   \n",
              "\n",
              "      reply_count  retweet_count  favorite_count lang          user_id_str  \\\n",
              "1494            0              0               0   in  1716092538833301504   \n",
              "1495            0              0               0   in  1694292049653366784   \n",
              "1496            0              0               0   in  1755409910593179648   \n",
              "1497            0              0               0   in  1356989898645929986   \n",
              "1498            0              0               0   in  1581628785815744512   \n",
              "\n",
              "      conversation_id_str         username  \\\n",
              "1494  1756934058377191563         Bebywoon   \n",
              "1495  1756933857834856732        ohyaagitu   \n",
              "1496  1756933425402151202  AzazilAzaz72537   \n",
              "1497  1756933267406606619     nobitatakesi   \n",
              "1498  1756933015492812906        Aditaahma   \n",
              "\n",
              "                                              tweet_url  masked_word_count  \\\n",
              "1494  https://twitter.com/Bebywoon/status/1756934058...                  0   \n",
              "1495  https://twitter.com/ohyaagitu/status/175693385...                  0   \n",
              "1496  https://twitter.com/AzazilAzaz72537/status/175...                  0   \n",
              "1497  https://twitter.com/nobitatakesi/status/175693...                  0   \n",
              "1498  https://twitter.com/Aditaahma/status/175693301...                  0   \n",
              "\n",
              "     pattern_counts  \n",
              "1494             {}  \n",
              "1495             {}  \n",
              "1496             {}  \n",
              "1497             {}  \n",
              "1498             {}  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_pemilu.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDvF724DmrDi"
      },
      "source": [
        "### Slang dan Abreviasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bJew3ZD2fsVP"
      },
      "outputs": [],
      "source": [
        "kamus_slang=pd.read_csv(r'src\\colloquial-indonesian-lexicon.csv')\n",
        "kamus_slang=kamus_slang.rename(columns = {'slang' : 'kamus_slang' , 'formal' : 'kamus_perbaikan'})\n",
        "\n",
        "# Rekonstruksi data sebagai 'dict'\n",
        "slang_mapping = dict(zip(kamus_slang['kamus_slang'], kamus_slang['kamus_perbaikan']))\n",
        "kamus_singkatan = pd.read_csv(r'src\\kamus_singkatan.csv', header=None, names=['sebelum_perbaikan', 'setelah_perbaikan'],delimiter=';')\n",
        "singkatan_mapping=dict(zip(kamus_singkatan['sebelum_perbaikan'],kamus_singkatan['setelah_perbaikan']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkUcqaK5myg9"
      },
      "source": [
        "### Stopword, emoji, dan Stemmer Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9KdjU3vdiG8J"
      },
      "outputs": [],
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import  StopWordRemoverFactory\n",
        "import emoji\n",
        "from spacy.lang.id import Indonesian\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yMqvORsOh_bN"
      },
      "outputs": [],
      "source": [
        "stopword_factory = StopWordRemoverFactory()\n",
        "stopwords = stopword_factory.get_stop_words()\n",
        "# List of words with negation meaning\n",
        "emoji_data = emoji.EMOJI_DATA\n",
        "\n",
        "# Remove negation words from stopwords\n",
        "# stopwords = set(stopwords).difference(excluded_stopwords)\n",
        "nlp = Indonesian()\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yHYIcUulqlkd"
      },
      "outputs": [],
      "source": [
        "def replace_emoji_with_ascii(text, emoji_data, language='id'):\n",
        "    for emoji, translations in emoji_data.items():\n",
        "        if language in translations:\n",
        "            text = text.replace(emoji, translations[language])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htCsjSPpxcbq",
        "outputId": "45abd325-3495-44de-d8df-f57272722bf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saat kamu merenungkan tentang kehilangan yang pernah kamu alami, lukarusluka itu terasa kembali dalam ingatan. patahmaskhati mekar #RememberingLoss\n"
          ]
        }
      ],
      "source": [
        "text_with_emoji = \"Saat kamu merenungkan tentang kehilangan yang pernah kamu alami, luka-luka itu terasa kembali dalam ingatan. ðŸ’”ðŸŒ¼ #RememberingLoss\"\n",
        "a = replace_emoji_with_ascii(text_with_emoji, emoji_data, language='id')\n",
        "a = a.replace(\":\",' ').replace('_','mask').replace('-','rus').strip()\n",
        "a = re.sub(' +', ' ', a)\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "A_fwSAIGlHjR"
      },
      "outputs": [],
      "source": [
        "def process_tweet(tweet) :\n",
        "  tweet=tweet.lower()\n",
        "  # link\n",
        "  tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
        "\n",
        "  # spesifik\n",
        "  # tweet = re.sub(r'\\[username\\]|\\[url\\]|\\[askmf\\]|\\[sensitive-no\\]|\\[satu menit kemudian\\]|\\[c48\\]|\\[idm\\]', '', tweet)\n",
        "\n",
        "  # emoji\n",
        "  tweet=replace_emoji_with_ascii(tweet,emoji_data)\n",
        "  tweet=tweet.replace(\":\",' ').replace('_','mask').replace('-','rus').strip()\n",
        "  tweet=re.sub(' +', ' ', tweet)\n",
        "\n",
        "  # tokenisasi\n",
        "  tokens = tweet.split()\n",
        "\n",
        "  tweet_tokens = []\n",
        "  for ele in tokens:\n",
        "    ele_kamus = kamus_singkatan.get(ele, ele)\n",
        "    ele_slang = slang_mapping.get(ele_kamus, ele_kamus)\n",
        "    tweet_tokens.append(ele_slang)\n",
        "\n",
        "  tweet = ' '.join(tweet_tokens)\n",
        "  tweet = re.sub('[\\s]+', ' ', tweet)\n",
        "    #Replace #word with word\n",
        "  tweet = re.sub(r'#([^\\s]+)', '', tweet)\n",
        "  tweet=re.sub(r'\\d+', '', tweet)\n",
        "  tweet = tweet.strip('\\'\"')\n",
        "  tweet = tweet.lstrip('\\'\"')\n",
        "\n",
        "  tweet = \"\".join([char for char in tweet if char not in string.punctuation])\n",
        "\n",
        "  doc = nlp(tweet)\n",
        "\n",
        "  tokens = [token.text for token in doc]\n",
        "      # Hapus stopwords dari tokens\n",
        "  filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "  tweet = ' '.join(filtered_tokens)\n",
        "\n",
        "  tweet=stemmer.stem(tweet)\n",
        "  tweet=tweet.replace('mask',' ').replace('rus','-')\n",
        "\n",
        "  return tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tQAIMjZJNnP",
        "outputId": "d2c232dd-2ed7-4580-e074-f19c2a30029a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hai sayang wajah gembira berurai air mata\n"
          ]
        }
      ],
      "source": [
        "print(process_tweet('hai sayangnya adalah ðŸ˜‚'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIJ3Ngn6Ip-y",
        "outputId": "ca386777-c271-45fd-fd2b-fc2ef166c137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "belialrising turun indon tengok je lah indon gi pwtc kelmarin macam mana perangai fanatik riuh macam sama je btw tak pergi milu presiden\n"
          ]
        }
      ],
      "source": [
        "print(process_tweet(str(df_pemilu['full_text'][0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qWWlAFV6omNo",
        "outputId": "5fe165f1-0e67-44ab-85e1-83fd1e8df4b3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_at</th>\n",
              "      <th>id_str</th>\n",
              "      <th>full_text</th>\n",
              "      <th>quote_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>lang</th>\n",
              "      <th>user_id_str</th>\n",
              "      <th>conversation_id_str</th>\n",
              "      <th>username</th>\n",
              "      <th>tweet_url</th>\n",
              "      <th>masked_word_count</th>\n",
              "      <th>pattern_counts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tue Feb 13 23:57:50 +0000 2024</td>\n",
              "      <td>1757554706011009374</td>\n",
              "      <td>@belialrising Keturunan indon? Tgk je la indon...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>in</td>\n",
              "      <td>1623528833793265665</td>\n",
              "      <td>1757325132274819532</td>\n",
              "      <td>reform_roar</td>\n",
              "      <td>https://twitter.com/reform_roar/status/1757554...</td>\n",
              "      <td>0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tue Feb 13 23:55:54 +0000 2024</td>\n",
              "      <td>1757554220851662851</td>\n",
              "      <td>Calon wakil presiden (cawapres) nomor urut 1, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>in</td>\n",
              "      <td>280794125</td>\n",
              "      <td>1757554220851662851</td>\n",
              "      <td>kalimaya_malang</td>\n",
              "      <td>https://twitter.com/kalimaya_malang/status/175...</td>\n",
              "      <td>0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tue Feb 13 23:55:48 +0000 2024</td>\n",
              "      <td>1757554194138157113</td>\n",
              "      <td>#02JariUngu sore sudah ketauan hasil pemilu pr...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>in</td>\n",
              "      <td>1402500059929800708</td>\n",
              "      <td>1757554194138157113</td>\n",
              "      <td>lalalalisae_m</td>\n",
              "      <td>https://twitter.com/lalalalisae_m/status/17575...</td>\n",
              "      <td>0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tue Feb 13 23:52:42 +0000 2024</td>\n",
              "      <td>1757553415172059371</td>\n",
              "      <td>siapapun bapak-bapak yang terpilih menjadi pre...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>in</td>\n",
              "      <td>1410553792865607686</td>\n",
              "      <td>1757553415172059371</td>\n",
              "      <td>caastagfirullah</td>\n",
              "      <td>https://twitter.com/caastagfirullah/status/175...</td>\n",
              "      <td>0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tue Feb 13 23:50:05 +0000 2024</td>\n",
              "      <td>1757552758109163895</td>\n",
              "      <td>@HisyamMochtar Pada tahun 20(24-Umat), rakyat ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>in</td>\n",
              "      <td>164950723</td>\n",
              "      <td>1757368895965729041</td>\n",
              "      <td>eindro76</td>\n",
              "      <td>https://twitter.com/eindro76/status/1757552758...</td>\n",
              "      <td>0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       created_at               id_str  \\\n",
              "0  Tue Feb 13 23:57:50 +0000 2024  1757554706011009374   \n",
              "1  Tue Feb 13 23:55:54 +0000 2024  1757554220851662851   \n",
              "2  Tue Feb 13 23:55:48 +0000 2024  1757554194138157113   \n",
              "3  Tue Feb 13 23:52:42 +0000 2024  1757553415172059371   \n",
              "4  Tue Feb 13 23:50:05 +0000 2024  1757552758109163895   \n",
              "\n",
              "                                           full_text  quote_count  \\\n",
              "0  @belialrising Keturunan indon? Tgk je la indon...            0   \n",
              "1  Calon wakil presiden (cawapres) nomor urut 1, ...            0   \n",
              "2  #02JariUngu sore sudah ketauan hasil pemilu pr...            0   \n",
              "3  siapapun bapak-bapak yang terpilih menjadi pre...            0   \n",
              "4  @HisyamMochtar Pada tahun 20(24-Umat), rakyat ...            0   \n",
              "\n",
              "   reply_count  retweet_count  favorite_count lang          user_id_str  \\\n",
              "0            1              1               1   in  1623528833793265665   \n",
              "1            0              0               1   in            280794125   \n",
              "2            0              0               1   in  1402500059929800708   \n",
              "3            0              0               0   in  1410553792865607686   \n",
              "4            0              0               1   in            164950723   \n",
              "\n",
              "   conversation_id_str         username  \\\n",
              "0  1757325132274819532      reform_roar   \n",
              "1  1757554220851662851  kalimaya_malang   \n",
              "2  1757554194138157113    lalalalisae_m   \n",
              "3  1757553415172059371  caastagfirullah   \n",
              "4  1757368895965729041         eindro76   \n",
              "\n",
              "                                           tweet_url  masked_word_count  \\\n",
              "0  https://twitter.com/reform_roar/status/1757554...                  0   \n",
              "1  https://twitter.com/kalimaya_malang/status/175...                  0   \n",
              "2  https://twitter.com/lalalalisae_m/status/17575...                  0   \n",
              "3  https://twitter.com/caastagfirullah/status/175...                  0   \n",
              "4  https://twitter.com/eindro76/status/1757552758...                  0   \n",
              "\n",
              "  pattern_counts  \n",
              "0             {}  \n",
              "1             {}  \n",
              "2             {}  \n",
              "3             {}  \n",
              "4             {}  "
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_pemilu.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "rXqpk-wRo2TZ"
      },
      "outputs": [],
      "source": [
        "df_emosi['tweet'] = df_emosi['tweet'].apply(lambda x: process_tweet(str(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Lzg2VV3WrNAX"
      },
      "outputs": [],
      "source": [
        "df_emosi.to_csv(r'src\\cleaned.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "E7CxM3omsCsG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       soal jalan jatibarupolisi gertak gubernur eman...\n",
              "1       sama cewek lho kayak ha- lebih rasai lah sibuk...\n",
              "2       pengin gudeg mbarek bu hj foto google sengaja ...\n",
              "3       jalan jatibarubagian wilayah tn abangpengatura...\n",
              "4       sharing alam aja kemarin jam batalin tiket sta...\n",
              "                              ...                        \n",
              "4396    tahu kamu papa mejam mata tahan gejolak batin ...\n",
              "4397    sulit tetap calon wapresnya jokowi pilpres sal...\n",
              "4398    masa depan enggak jelas lah iya bagaimana mau ...\n",
              "4399    dulu benar mahasiswa teknik ui tembak pacar pa...\n",
              "4400       allah engkau tahu rasa sakit hati sembuh allah\n",
              "Name: tweet, Length: 4401, dtype: object"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_emosi['tweet']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Try to use each representation separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X = df_emosi['tweet']\n",
        "y = df_emosi['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1769    kaget berita tetangga satu rt solo tahun tingg...\n",
              "1220          tidak-enak sangat bada pulang kerjan banyak\n",
              "44      iya ibu nya lahir anak cewek enggak tahu tahun...\n",
              "289     jiyeeee jiyeee jeng dom habis menang lawan kei...\n",
              "2486    cinta penuh banyak buat semua harga tak satu l...\n",
              "                              ...                        \n",
              "3444    sahabat perlu filosopi milik nilai hidup beri ...\n",
              "466     banyak bilang pilih sopir mobil bener bawa mas...\n",
              "3092    bilang tetap pegang janji nikah brhak hakim su...\n",
              "3772    allahapa kok disalahin pres kayak presiden kur...\n",
              "860     gue punya teman dibela-belain pinjem duwit kan...\n",
              "Name: tweet, Length: 3520, dtype: object"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bag of Words\n",
        "vectorizer_bow = CountVectorizer()\n",
        "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
        "X_test_bow = vectorizer_bow.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 4810)\t1\n",
            "  (0, 1147)\t1\n",
            "  (0, 10739)\t1\n",
            "  (0, 9318)\t1\n",
            "  (0, 9102)\t1\n",
            "  (0, 10019)\t1\n",
            "  (0, 10380)\t1\n",
            "  (0, 10859)\t1\n",
            "  (0, 2700)\t1\n",
            "  (0, 11153)\t1\n",
            "  (0, 346)\t1\n",
            "  (0, 349)\t1\n",
            "  (0, 7638)\t1\n",
            "  (0, 1087)\t2\n",
            "  (0, 4210)\t1\n",
            "  (0, 5970)\t1\n",
            "  (0, 11245)\t1\n",
            "  (0, 4273)\t1\n",
            "  (0, 11411)\t1\n",
            "  (0, 4271)\t1\n",
            "  (0, 9066)\t1\n",
            "  (0, 9605)\t1\n",
            "  (0, 253)\t1\n",
            "  (0, 3033)\t1\n",
            "  (1, 10821)\t1\n",
            "  :\t:\n",
            "  (3518, 2483)\t1\n",
            "  (3518, 3202)\t1\n",
            "  (3518, 10910)\t1\n",
            "  (3518, 299)\t1\n",
            "  (3518, 8549)\t1\n",
            "  (3518, 3138)\t1\n",
            "  (3518, 4641)\t1\n",
            "  (3519, 1504)\t1\n",
            "  (3519, 3607)\t1\n",
            "  (3519, 10605)\t1\n",
            "  (3519, 10805)\t1\n",
            "  (3519, 8681)\t1\n",
            "  (3519, 10477)\t1\n",
            "  (3519, 9321)\t1\n",
            "  (3519, 4914)\t1\n",
            "  (3519, 8358)\t2\n",
            "  (3519, 11337)\t1\n",
            "  (3519, 5188)\t1\n",
            "  (3519, 7640)\t1\n",
            "  (3519, 470)\t1\n",
            "  (3519, 7951)\t1\n",
            "  (3519, 3390)\t1\n",
            "  (3519, 2290)\t1\n",
            "  (3519, 1050)\t1\n",
            "  (3519, 2713)\t1\n"
          ]
        }
      ],
      "source": [
        "print(X_train_bow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer_tfidf.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 3033)\t0.22954952181025595\n",
            "  (0, 253)\t0.1820641846385591\n",
            "  (0, 9605)\t0.2500003433823169\n",
            "  (0, 9066)\t0.23803737965372196\n",
            "  (0, 4271)\t0.2500003433823169\n",
            "  (0, 11411)\t0.1721368072098088\n",
            "  (0, 4273)\t0.22954952181025595\n",
            "  (0, 11245)\t0.1672891608802246\n",
            "  (0, 5970)\t0.10814039858178676\n",
            "  (0, 4210)\t0.17432331096844592\n",
            "  (0, 1087)\t0.35588121304178577\n",
            "  (0, 7638)\t0.2130384503539307\n",
            "  (0, 349)\t0.1721368072098088\n",
            "  (0, 346)\t0.2500003433823169\n",
            "  (0, 11153)\t0.2500003433823169\n",
            "  (0, 2700)\t0.14427112963688324\n",
            "  (0, 10859)\t0.13868767712190488\n",
            "  (0, 10380)\t0.1268379737967491\n",
            "  (0, 10019)\t0.1997029461808805\n",
            "  (0, 9102)\t0.20251500621062005\n",
            "  (0, 9318)\t0.12241984116408033\n",
            "  (0, 10739)\t0.18864787866613406\n",
            "  (0, 1147)\t0.18357755732589456\n",
            "  (0, 4810)\t0.17925212460881954\n",
            "  (1, 895)\t0.22239438583203394\n",
            "  :\t:\n",
            "  (3518, 1532)\t0.15131114175447472\n",
            "  (3518, 8554)\t0.2080624342218105\n",
            "  (3518, 5214)\t0.1677000294324275\n",
            "  (3518, 5428)\t0.15604174579959643\n",
            "  (3518, 495)\t0.12332068707344476\n",
            "  (3518, 1499)\t0.20312081639137167\n",
            "  (3518, 5010)\t0.14152874735076804\n",
            "  (3519, 2713)\t0.2621542090294831\n",
            "  (3519, 1050)\t0.2621542090294831\n",
            "  (3519, 2290)\t0.2621542090294831\n",
            "  (3519, 3390)\t0.22816461472660593\n",
            "  (3519, 7951)\t0.24070916267197498\n",
            "  (3519, 470)\t0.21236035339582315\n",
            "  (3519, 7640)\t0.249609661084114\n",
            "  (3519, 5188)\t0.249609661084114\n",
            "  (3519, 11337)\t0.23380539975333123\n",
            "  (3519, 8358)\t0.499219322168228\n",
            "  (3519, 4914)\t0.1718822864638018\n",
            "  (3519, 9321)\t0.187966537597177\n",
            "  (3519, 10477)\t0.23380539975333123\n",
            "  (3519, 8681)\t0.12259425128527934\n",
            "  (3519, 10805)\t0.1978190699569588\n",
            "  (3519, 10605)\t0.12878385180301702\n",
            "  (3519, 3607)\t0.09925356756853812\n",
            "  (3519, 1504)\t0.10047659882865577\n"
          ]
        }
      ],
      "source": [
        "print(X_train_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# N-grams (Unigram and Bigram)\n",
        "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2))\n",
        "X_train_ngram = vectorizer_ngram.fit_transform(X_train)\n",
        "X_test_ngram = vectorizer_ngram.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 27918)\t1\n",
            "  (0, 8289)\t1\n",
            "  (0, 64020)\t1\n",
            "  (0, 54790)\t1\n",
            "  (0, 52951)\t1\n",
            "  (0, 59578)\t1\n",
            "  (0, 61327)\t1\n",
            "  (0, 64775)\t1\n",
            "  (0, 16602)\t1\n",
            "  (0, 66347)\t1\n",
            "  (0, 2330)\t1\n",
            "  (0, 2338)\t1\n",
            "  (0, 45157)\t1\n",
            "  (0, 7900)\t2\n",
            "  (0, 24271)\t1\n",
            "  (0, 35879)\t1\n",
            "  (0, 66768)\t1\n",
            "  (0, 24631)\t1\n",
            "  (0, 67551)\t1\n",
            "  (0, 24627)\t1\n",
            "  (0, 52845)\t1\n",
            "  (0, 57176)\t1\n",
            "  (0, 1787)\t1\n",
            "  (0, 18380)\t1\n",
            "  (0, 27922)\t1\n",
            "  :\t:\n",
            "  (3519, 20950)\t1\n",
            "  (3519, 31517)\t1\n",
            "  (3519, 45166)\t1\n",
            "  (3519, 45167)\t1\n",
            "  (3519, 3030)\t1\n",
            "  (3519, 47287)\t1\n",
            "  (3519, 19734)\t1\n",
            "  (3519, 14867)\t1\n",
            "  (3519, 7467)\t1\n",
            "  (3519, 16691)\t1\n",
            "  (3519, 63181)\t1\n",
            "  (3519, 14868)\t1\n",
            "  (3519, 7468)\t1\n",
            "  (3519, 49755)\t1\n",
            "  (3519, 16692)\t1\n",
            "  (3519, 29570)\t1\n",
            "  (3519, 10370)\t1\n",
            "  (3519, 64344)\t1\n",
            "  (3519, 49757)\t1\n",
            "  (3519, 67272)\t1\n",
            "  (3519, 19735)\t1\n",
            "  (3519, 3034)\t1\n",
            "  (3519, 31519)\t1\n",
            "  (3519, 47290)\t1\n",
            "  (3519, 62219)\t1\n"
          ]
        }
      ],
      "source": [
        "print(X_train_ngram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes with Bag of Words\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.71      0.79      0.74       229\n",
            "        fear       0.72      0.64      0.68       119\n",
            "       happy       0.73      0.62      0.67       214\n",
            "        love       0.74      0.73      0.73       119\n",
            "     sadness       0.53      0.58      0.56       200\n",
            "\n",
            "    accuracy                           0.67       881\n",
            "   macro avg       0.68      0.67      0.68       881\n",
            "weighted avg       0.68      0.67      0.67       881\n",
            "\n",
            "Naive Bayes with TF-IDF\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.61      0.84      0.71       229\n",
            "        fear       0.97      0.29      0.44       119\n",
            "       happy       0.71      0.62      0.66       214\n",
            "        love       0.87      0.40      0.55       119\n",
            "     sadness       0.44      0.65      0.53       200\n",
            "\n",
            "    accuracy                           0.61       881\n",
            "   macro avg       0.72      0.56      0.58       881\n",
            "weighted avg       0.68      0.61      0.60       881\n",
            "\n",
            "Naive Bayes with N-grams\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.66      0.81      0.73       229\n",
            "        fear       0.82      0.63      0.71       119\n",
            "       happy       0.70      0.62      0.66       214\n",
            "        love       0.78      0.68      0.73       119\n",
            "     sadness       0.56      0.60      0.58       200\n",
            "\n",
            "    accuracy                           0.67       881\n",
            "   macro avg       0.70      0.67      0.68       881\n",
            "weighted avg       0.68      0.67      0.67       881\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Naive Bayes\n",
        "nb = MultinomialNB()\n",
        "\n",
        "# Bag of Words\n",
        "nb.fit(X_train_bow, y_train)\n",
        "y_pred_bow_nb = nb.predict(X_test_bow)\n",
        "print(\"Naive Bayes with Bag of Words\")\n",
        "print(classification_report(y_test, y_pred_bow_nb))\n",
        "\n",
        "# TF-IDF\n",
        "nb.fit(X_train_tfidf, y_train)\n",
        "y_pred_tfidf_nb = nb.predict(X_test_tfidf)\n",
        "print(\"Naive Bayes with TF-IDF\")\n",
        "print(classification_report(y_test, y_pred_tfidf_nb))\n",
        "\n",
        "# N-grams\n",
        "nb.fit(X_train_ngram, y_train)\n",
        "y_pred_ngram_nb = nb.predict(X_test_ngram)\n",
        "print(\"Naive Bayes with N-grams\")\n",
        "print(classification_report(y_test, y_pred_ngram_nb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with Bag of Words\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.54      0.76      0.64       229\n",
            "        fear       0.86      0.63      0.73       119\n",
            "       happy       0.71      0.57      0.63       214\n",
            "        love       0.66      0.82      0.73       119\n",
            "     sadness       0.53      0.41      0.46       200\n",
            "\n",
            "    accuracy                           0.62       881\n",
            "   macro avg       0.66      0.64      0.64       881\n",
            "weighted avg       0.64      0.62      0.62       881\n",
            "\n",
            "Random Forest with TF-IDF\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.56      0.76      0.64       229\n",
            "        fear       0.83      0.61      0.71       119\n",
            "       happy       0.72      0.57      0.63       214\n",
            "        love       0.67      0.79      0.72       119\n",
            "     sadness       0.49      0.41      0.45       200\n",
            "\n",
            "    accuracy                           0.62       881\n",
            "   macro avg       0.65      0.63      0.63       881\n",
            "weighted avg       0.63      0.62      0.62       881\n",
            "\n",
            "Random Forest with N-grams\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.53      0.80      0.64       229\n",
            "        fear       0.87      0.60      0.71       119\n",
            "       happy       0.71      0.57      0.63       214\n",
            "        love       0.70      0.79      0.74       119\n",
            "     sadness       0.53      0.39      0.45       200\n",
            "\n",
            "    accuracy                           0.62       881\n",
            "   macro avg       0.67      0.63      0.63       881\n",
            "weighted avg       0.64      0.62      0.62       881\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Random Forest\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Bag of Words\n",
        "rf.fit(X_train_bow, y_train)\n",
        "y_pred_bow_rf = rf.predict(X_test_bow)\n",
        "print(\"Random Forest with Bag of Words\")\n",
        "print(classification_report(y_test, y_pred_bow_rf))\n",
        "\n",
        "# TF-IDF\n",
        "rf.fit(X_train_tfidf, y_train)\n",
        "y_pred_tfidf_rf = rf.predict(X_test_tfidf)\n",
        "print(\"Random Forest with TF-IDF\")\n",
        "print(classification_report(y_test, y_pred_tfidf_rf))\n",
        "\n",
        "# N-grams\n",
        "rf.fit(X_train_ngram, y_train)\n",
        "y_pred_ngram_rf = rf.predict(X_test_ngram)\n",
        "print(\"Random Forest with N-grams\")\n",
        "print(classification_report(y_test, y_pred_ngram_rf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM with Bag of Words\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.58      0.77      0.67       229\n",
            "        fear       0.87      0.51      0.65       119\n",
            "       happy       0.62      0.63      0.62       214\n",
            "        love       0.75      0.72      0.74       119\n",
            "     sadness       0.50      0.43      0.46       200\n",
            "\n",
            "    accuracy                           0.62       881\n",
            "   macro avg       0.66      0.61      0.63       881\n",
            "weighted avg       0.63      0.62      0.62       881\n",
            "\n",
            "SVM with TF-IDF\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.62      0.81      0.70       229\n",
            "        fear       0.88      0.55      0.67       119\n",
            "       happy       0.66      0.65      0.66       214\n",
            "        love       0.79      0.70      0.74       119\n",
            "     sadness       0.53      0.51      0.52       200\n",
            "\n",
            "    accuracy                           0.65       881\n",
            "   macro avg       0.70      0.64      0.66       881\n",
            "weighted avg       0.67      0.65      0.65       881\n",
            "\n",
            "SVM with N-grams\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.59      0.75      0.66       229\n",
            "        fear       0.84      0.49      0.62       119\n",
            "       happy       0.59      0.64      0.61       214\n",
            "        love       0.75      0.76      0.75       119\n",
            "     sadness       0.50      0.42      0.46       200\n",
            "\n",
            "    accuracy                           0.61       881\n",
            "   macro avg       0.65      0.61      0.62       881\n",
            "weighted avg       0.63      0.61      0.61       881\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Support Vector Machine\n",
        "svm = SVC()\n",
        "\n",
        "# Bag of Words\n",
        "svm.fit(X_train_bow, y_train)\n",
        "y_pred_bow_svm = svm.predict(X_test_bow)\n",
        "print(\"SVM with Bag of Words\")\n",
        "print(classification_report(y_test, y_pred_bow_svm))\n",
        "\n",
        "# TF-IDF\n",
        "svm.fit(X_train_tfidf, y_train)\n",
        "y_pred_tfidf_svm = svm.predict(X_test_tfidf)\n",
        "print(\"SVM with TF-IDF\")\n",
        "print(classification_report(y_test, y_pred_tfidf_svm))\n",
        "\n",
        "# N-grams\n",
        "svm.fit(X_train_ngram, y_train)\n",
        "y_pred_ngram_svm = svm.predict(X_test_ngram)\n",
        "print(\"SVM with N-grams\")\n",
        "print(classification_report(y_test, y_pred_ngram_svm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## use three representation in one model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultinomialNB with Combined Features\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.67      0.82      0.73       229\n",
            "        fear       0.80      0.62      0.70       119\n",
            "       happy       0.71      0.62      0.66       214\n",
            "        love       0.77      0.69      0.73       119\n",
            "     sadness       0.53      0.58      0.56       200\n",
            "\n",
            "    accuracy                           0.67       881\n",
            "   macro avg       0.70      0.66      0.68       881\n",
            "weighted avg       0.68      0.67      0.67       881\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define the feature extraction steps\n",
        "vectorizer_bow = CountVectorizer()\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2))\n",
        "\n",
        "# Combine the features using FeatureUnion\n",
        "combined_features = FeatureUnion([\n",
        "    (\"bow\", vectorizer_bow),\n",
        "    (\"tfidf\", vectorizer_tfidf),\n",
        "    (\"ngram\", vectorizer_ngram)\n",
        "])\n",
        "\n",
        "# Create a pipeline that first transforms the data and then applies the model\n",
        "pipeline = Pipeline([\n",
        "    (\"features\", combined_features),\n",
        "    (\"classifier\", MultinomialNB())  # You can replace MultinomialNB with any other classifier\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(\"MultinomialNB with Combined Features\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with Combined Features\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.52      0.79      0.62       229\n",
            "        fear       0.86      0.63      0.73       119\n",
            "       happy       0.69      0.55      0.61       214\n",
            "        love       0.67      0.81      0.73       119\n",
            "     sadness       0.55      0.36      0.43       200\n",
            "\n",
            "    accuracy                           0.61       881\n",
            "   macro avg       0.66      0.63      0.63       881\n",
            "weighted avg       0.63      0.61      0.61       881\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pipeline_rf = Pipeline([\n",
        "    (\"features\", combined_features),\n",
        "    (\"classifier\", RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline_rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_rf = pipeline_rf.predict(X_test)\n",
        "print(\"Random Forest with Combined Features\")\n",
        "print(classification_report(y_test, y_pred_rf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM with Combined Features\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.58      0.75      0.66       229\n",
            "        fear       0.86      0.50      0.63       119\n",
            "       happy       0.60      0.63      0.62       214\n",
            "        love       0.75      0.75      0.75       119\n",
            "     sadness       0.49      0.42      0.45       200\n",
            "\n",
            "    accuracy                           0.61       881\n",
            "   macro avg       0.65      0.61      0.62       881\n",
            "weighted avg       0.62      0.61      0.61       881\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pipeline_svm = Pipeline([\n",
        "    (\"features\", combined_features),\n",
        "    (\"classifier\", SVC())\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline_svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_svm = pipeline_svm.predict(X_test)\n",
        "print(\"SVM with Combined Features\")\n",
        "print(classification_report(y_test, y_pred_svm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPsXqn5SoCzv"
      },
      "source": [
        "### Clean Data ke Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BasV1X1qIay"
      },
      "outputs": [],
      "source": [
        "# data_clean_url='https://drive.google.com/uc?id=13S8a6HUKDAnJNCmxOS0Yy-pqMZGLCfl7'\n",
        "# data_clean_path='data_clean.csv'\n",
        "# gdown.download(data_clean_url, data_clean_path, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OdWjhiUqeiz"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data_clean.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRW2nHLXvQtb"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "# from gensim.model import CoherenceModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L93vklZ3vfQT"
      },
      "outputs": [],
      "source": [
        "print(gensim.utils.simple_preprocess('mendagri terbit susul cabut bijak laku batas giat masyarakat ppkm presiden', deacc=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "635nHCQsuf7b"
      },
      "outputs": [],
      "source": [
        "def gen_words(texts):\n",
        "  final = []\n",
        "  for text in texts:\n",
        "    new = gensim.utils.simple_preprocess(text, deacc=True)\n",
        "    final.append(new)\n",
        "  return final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUGpFC1tv5PF"
      },
      "outputs": [],
      "source": [
        "texts = gen_words(df['full_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx-WbsW4wFcY"
      },
      "outputs": [],
      "source": [
        "print(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4u5C1qCwHJM"
      },
      "outputs": [],
      "source": [
        "id2word = corpora.Dictionary(texts)\n",
        "\n",
        "corpus = []\n",
        "for text in texts:\n",
        "  new = id2word.doc2bow(text)\n",
        "  corpus.append(new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKN9IBvcoReY"
      },
      "source": [
        "### LDA Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEeTlAJ5x7Lw"
      },
      "outputs": [],
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                            id2word=id2word,\n",
        "                                            num_topics=12,\n",
        "                                            random_state=42,\n",
        "                                            update_every=1,\n",
        "                                            chunksize=100,\n",
        "                                            alpha='auto'\n",
        "                                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ip-ys6uoaY0"
      },
      "source": [
        "### LDA Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlxHbMu7yqrg"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1yi-B2ayguH"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GYbIRpuy5QC"
      },
      "outputs": [],
      "source": [
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds='mmds', R=30)\n",
        "vis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-mwTib9odAE"
      },
      "source": [
        "### Dynamic LDA Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTOYPRSHzO4t"
      },
      "outputs": [],
      "source": [
        "ldaseq = gensim.models.ldaseqmodel.LdaSeqModel(corpus=corpus,\n",
        "                                 id2word=id2word,\n",
        "                                 time_slice=[503,503,503,503],\n",
        "                                 num_topics=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMM7EsgerRNQ"
      },
      "outputs": [],
      "source": [
        "ldaseq.print_topics(time=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk53HFB4rcsr"
      },
      "outputs": [],
      "source": [
        "ldaseq.print_topic_times(topic=2) # evolution of 1st topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT6B5TPXtZmG"
      },
      "outputs": [],
      "source": [
        "ldaseq.save(\"/content/modelseq.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7W00UAH35dNn"
      },
      "outputs": [],
      "source": [
        "seq_model =  gensim.models.LdaSeqModel.load('/content/modelseq.model')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qKN9IBvcoReY",
        "1ip-ys6uoaY0"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
