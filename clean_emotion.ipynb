{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ebUIan1kWjuC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "import re\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cbpZout6E5rg"
      },
      "outputs": [],
      "source": [
        "df_emosi=pd.read_csv(r'src\\Twitter_Emotion_Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-2R82OCCzTMQ"
      },
      "outputs": [],
      "source": [
        "df_pemilu=pd.read_csv(r'src\\pemilu-2024.csv', on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baIkTC_gIQlP"
      },
      "source": [
        "### Missval Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GebdnPDcIU5x",
        "outputId": "9ec5fb91-0d22-4fe0-d953-a275b41ba2c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label    0\n",
              "tweet    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_emosi.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfO2J75Z0Yjb"
      },
      "source": [
        "### Masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7ICp5M80bpm",
        "outputId": "313d0723-cc80-42a5-be87-189ad474cb75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total kata yang dimask dalam dataset: 1793\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk menghitung kata yang dimask dalam sebuah teks\n",
        "def count_masked_words(text):\n",
        "    if isinstance(text, str):\n",
        "        pattern = r'\\[USERNAME\\]|\\[USER\\]'  # Pola regex untuk mencari [USERNAME] atau [USER]\n",
        "        matches = re.findall(pattern, text)\n",
        "        return len(matches)\n",
        "    else:\n",
        "        return 0  # Jika nilai tidak bertipe string, kembalikan 0\n",
        "\n",
        "# Menerapkan fungsi ke kolom teks dalam DataFrame\n",
        "df_emosi['masked_word_count'] = df_emosi['tweet'].apply(count_masked_words)\n",
        "\n",
        "# Jumlah kata yang dimask dalam keseluruhan dataset\n",
        "total_masked_words = df_emosi['masked_word_count'].sum()\n",
        "\n",
        "print(\"Total kata yang dimask dalam dataset:\", total_masked_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Wrfg6wLTJ_pA"
      },
      "outputs": [],
      "source": [
        "# Fungsi untuk menemukan dan menghitung pola yang diawali dengan tanda kurung siku dalam sebuah teks\n",
        "def find_and_count_patterns(text):\n",
        "    if isinstance(text, str):\n",
        "        pattern = r'\\[([^]]+)\\]'  # Pola regex untuk mencari semua pola yang diawali dengan tanda kurung siku\n",
        "        matches = re.findall(pattern, text)\n",
        "\n",
        "        # Menghitung jumlah kemunculan setiap pola\n",
        "        pattern_counts = {}\n",
        "        for match in matches:\n",
        "            if match in pattern_counts:\n",
        "                pattern_counts[match] += 1\n",
        "            else:\n",
        "                pattern_counts[match] = 1\n",
        "\n",
        "        return pattern_counts\n",
        "    else:\n",
        "        return {}  # Jika nilai tidak bertipe string, kembalikan dictionary kosong\n",
        "\n",
        "# Menerapkan fungsi ke kolom teks dalam DataFrame\n",
        "df_emosi['pattern_counts'] = df_emosi['tweet'].apply(find_and_count_patterns)\n",
        "\n",
        "# Menggabungkan hasil dari semua tweet menjadi satu dictionary\n",
        "all_patterns_counts = {}\n",
        "for pattern_count in df_emosi['pattern_counts']:\n",
        "    for pattern, count in pattern_count.items():\n",
        "        if pattern in all_patterns_counts:\n",
        "            all_patterns_counts[pattern] += count\n",
        "        else:\n",
        "            all_patterns_counts[pattern] = count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNNmLuI8KM34",
        "outputId": "ee0aeafe-e883-4cb5-aced-bda63ff1c63d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USERNAME 1793\n",
            "URL 621\n",
            "askmf 10\n",
            "SENSITIVE-NO 5\n",
            "askMF 4\n",
            "Idm 2\n",
            "AskMF 2\n",
            "C48 2\n",
            "idm 1\n",
            "Seo In Ha, Love Rain 1\n",
            "Askmf 1\n",
            "Late Post 1\n",
            "Allamah Thabathabai 1\n",
            "Kartu 1 pria thd wanita 1\n",
            "Habis buka Facebook 1\n",
            "Thinking.. 1\n",
            "BELAJARLAH DEMI ORANGTUAMU!!! 1\n",
            "BB 1\n",
            "Obrolan dengan dospem 1 & 2 di grup WA menjelang besok sidang 1\n",
            "Satu menit kemudian 1\n",
            "1 1\n"
          ]
        }
      ],
      "source": [
        "# Membuat barplot\n",
        "all_patterns_counts = sorted(all_patterns_counts.items(), key=lambda x:x[1], reverse=True)\n",
        "all_patterns_counts = dict(all_patterns_counts)\n",
        "for k, i in all_patterns_counts.items():\n",
        "  print(k, i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kzPQlh4lwoPq",
        "outputId": "c2d9b3d9-bd41-4eb7-8312-d3c7f977c4d8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>masked_word_count</th>\n",
              "      <th>pattern_counts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4396</th>\n",
              "      <td>love</td>\n",
              "      <td>Tahukah kamu, bahwa saat itu papa memejamkan m...</td>\n",
              "      <td>0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4397</th>\n",
              "      <td>fear</td>\n",
              "      <td>Sulitnya menetapkan Calon Wapresnya Jokowi di ...</td>\n",
              "      <td>0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4398</th>\n",
              "      <td>anger</td>\n",
              "      <td>5. masa depannya nggak jelas. lha iya, gimana ...</td>\n",
              "      <td>0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4399</th>\n",
              "      <td>happy</td>\n",
              "      <td>[USERNAME] dulu beneran ada mahasiswa Teknik U...</td>\n",
              "      <td>1</td>\n",
              "      <td>{'USERNAME': 1}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4400</th>\n",
              "      <td>sadness</td>\n",
              "      <td>Ya Allah, hanya Engkau yang mengetahui rasa sa...</td>\n",
              "      <td>0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        label                                              tweet  \\\n",
              "4396     love  Tahukah kamu, bahwa saat itu papa memejamkan m...   \n",
              "4397     fear  Sulitnya menetapkan Calon Wapresnya Jokowi di ...   \n",
              "4398    anger  5. masa depannya nggak jelas. lha iya, gimana ...   \n",
              "4399    happy  [USERNAME] dulu beneran ada mahasiswa Teknik U...   \n",
              "4400  sadness  Ya Allah, hanya Engkau yang mengetahui rasa sa...   \n",
              "\n",
              "      masked_word_count   pattern_counts  \n",
              "4396                  0               {}  \n",
              "4397                  0               {}  \n",
              "4398                  0               {}  \n",
              "4399                  1  {'USERNAME': 1}  \n",
              "4400                  0               {}  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_emosi.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDvF724DmrDi"
      },
      "source": [
        "### Slang dan Abreviasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bJew3ZD2fsVP"
      },
      "outputs": [],
      "source": [
        "kamus_slang=pd.read_csv(r'src\\colloquial-indonesian-lexicon.csv')\n",
        "kamus_slang=kamus_slang.rename(columns = {'slang' : 'kamus_slang' , 'formal' : 'kamus_perbaikan'})\n",
        "\n",
        "# Rekonstruksi data sebagai 'dict'\n",
        "slang_mapping = dict(zip(kamus_slang['kamus_slang'], kamus_slang['kamus_perbaikan']))\n",
        "kamus_singkatan = pd.read_csv(r'src\\kamus_singkatan.csv', header=None, names=['sebelum_perbaikan', 'setelah_perbaikan'],delimiter=';')\n",
        "singkatan_mapping=dict(zip(kamus_singkatan['sebelum_perbaikan'],kamus_singkatan['setelah_perbaikan']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkUcqaK5myg9"
      },
      "source": [
        "### Stopword, emoji, dan Stemmer Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9KdjU3vdiG8J"
      },
      "outputs": [],
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import  StopWordRemoverFactory\n",
        "import emoji\n",
        "from spacy.lang.id import Indonesian\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "yMqvORsOh_bN"
      },
      "outputs": [],
      "source": [
        "stopword_factory = StopWordRemoverFactory()\n",
        "stopwords = stopword_factory.get_stop_words()\n",
        "# List of words with negation meaning\n",
        "emoji_data = emoji.EMOJI_DATA\n",
        "\n",
        "# Remove negation words from stopwords\n",
        "# stopwords = set(stopwords).difference(excluded_stopwords)\n",
        "nlp = Indonesian()\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yHYIcUulqlkd"
      },
      "outputs": [],
      "source": [
        "def replace_emoji_with_ascii(text, emoji_data, language='id'):\n",
        "    for emoji, translations in emoji_data.items():\n",
        "        if language in translations:\n",
        "            text = text.replace(emoji, translations[language])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htCsjSPpxcbq",
        "outputId": "45abd325-3495-44de-d8df-f57272722bf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saat kamu merenungkan tentang kehilangan yang pernah kamu alami, lukarusluka itu terasa kembali dalam ingatan. patahmaskhati mekar #RememberingLoss\n"
          ]
        }
      ],
      "source": [
        "text_with_emoji = \"Saat kamu merenungkan tentang kehilangan yang pernah kamu alami, luka-luka itu terasa kembali dalam ingatan. 💔🌼 #RememberingLoss\"\n",
        "a = replace_emoji_with_ascii(text_with_emoji, emoji_data, language='id')\n",
        "a = a.replace(\":\",' ').replace('_','mask').replace('-','rus').strip()\n",
        "a = re.sub(' +', ' ', a)\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "A_fwSAIGlHjR"
      },
      "outputs": [],
      "source": [
        "def process_tweet(tweet) :\n",
        "  tweet=tweet.lower()\n",
        "  # link\n",
        "  tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
        "\n",
        "  # spesifik\n",
        "  tweet = re.sub(r'\\[username\\]|\\[url\\]|\\[askmf\\]|\\[sensitive-no\\]|\\[satu menit kemudian\\]|\\[c48\\]|\\[idm\\]', '', tweet)\n",
        "\n",
        "  # emoji\n",
        "  tweet=replace_emoji_with_ascii(tweet,emoji_data)\n",
        "  tweet=tweet.replace(\":\",' ').replace('_','mask').replace('-','rus').strip()\n",
        "  tweet=re.sub(' +', ' ', tweet)\n",
        "\n",
        "  # tokenisasi\n",
        "  tokens = tweet.split()\n",
        "\n",
        "  tweet_tokens = []\n",
        "  for ele in tokens:\n",
        "    ele_kamus = kamus_singkatan.get(ele, ele)\n",
        "    ele_slang = slang_mapping.get(ele_kamus, ele_kamus)\n",
        "    tweet_tokens.append(ele_slang)\n",
        "\n",
        "  tweet = ' '.join(tweet_tokens)\n",
        "  tweet = re.sub('[\\s]+', ' ', tweet)\n",
        "    #Replace #word with word\n",
        "  tweet = re.sub(r'#([^\\s]+)', '', tweet)\n",
        "  tweet=re.sub(r'\\d+', '', tweet)\n",
        "  tweet = tweet.strip('\\'\"')\n",
        "  tweet = tweet.lstrip('\\'\"')\n",
        "\n",
        "  tweet = \"\".join([char for char in tweet if char not in string.punctuation])\n",
        "\n",
        "  doc = nlp(tweet)\n",
        "\n",
        "  tokens = [token.text for token in doc]\n",
        "      # Hapus stopwords dari tokens\n",
        "  filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "  tweet = ' '.join(filtered_tokens)\n",
        "\n",
        "  tweet=stemmer.stem(tweet)\n",
        "  tweet=tweet.replace('mask',' ').replace('rus','-')\n",
        "\n",
        "  return tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tQAIMjZJNnP",
        "outputId": "d2c232dd-2ed7-4580-e074-f19c2a30029a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hai sayang wajah gembira berurai air mata\n"
          ]
        }
      ],
      "source": [
        "print(process_tweet('hai sayangnya adalah 😂'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIJ3Ngn6Ip-y",
        "outputId": "ca386777-c271-45fd-fd2b-fc2ef166c137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "soal jalan jatibarupolisi gertak gubernur emangny polisi ikut pmbhasan jangan politik atur wilayahhak gubernur soal tn abang soal turun temurunpelikperlu sabar\n"
          ]
        }
      ],
      "source": [
        "print(process_tweet(str(df_emosi['tweet'][0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qWWlAFV6omNo",
        "outputId": "5fe165f1-0e67-44ab-85e1-83fd1e8df4b3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>masked_word_count</th>\n",
              "      <th>pattern_counts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>anger</td>\n",
              "      <td>Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...</td>\n",
              "      <td>2</td>\n",
              "      <td>{'USERNAME': 2, 'URL': 1}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>anger</td>\n",
              "      <td>Sesama cewe lho (kayaknya), harusnya bisa lebi...</td>\n",
              "      <td>0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>happy</td>\n",
              "      <td>Kepingin gudeg mbarek Bu hj. Amad Foto dari go...</td>\n",
              "      <td>0</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>anger</td>\n",
              "      <td>Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...</td>\n",
              "      <td>0</td>\n",
              "      <td>{'URL': 1}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>happy</td>\n",
              "      <td>Sharing pengalaman aja, kemarin jam 18.00 bata...</td>\n",
              "      <td>1</td>\n",
              "      <td>{'USERNAME': 1}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                              tweet  \\\n",
              "0  anger  Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...   \n",
              "1  anger  Sesama cewe lho (kayaknya), harusnya bisa lebi...   \n",
              "2  happy  Kepingin gudeg mbarek Bu hj. Amad Foto dari go...   \n",
              "3  anger  Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...   \n",
              "4  happy  Sharing pengalaman aja, kemarin jam 18.00 bata...   \n",
              "\n",
              "   masked_word_count             pattern_counts  \n",
              "0                  2  {'USERNAME': 2, 'URL': 1}  \n",
              "1                  0                         {}  \n",
              "2                  0                         {}  \n",
              "3                  0                 {'URL': 1}  \n",
              "4                  1            {'USERNAME': 1}  "
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_emosi.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "rXqpk-wRo2TZ"
      },
      "outputs": [],
      "source": [
        "df_emosi['tweet'] = df_emosi['tweet'].apply(lambda x: process_tweet(str(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Lzg2VV3WrNAX"
      },
      "outputs": [],
      "source": [
        "df_emosi.to_csv(r'src\\cleaned.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "E7CxM3omsCsG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       soal jalan jatibarupolisi gertak gubernur eman...\n",
              "1       sama cewek lho kayak ha- lebih rasai lah sibuk...\n",
              "2       pengin gudeg mbarek bu hj foto google sengaja ...\n",
              "3       jalan jatibarubagian wilayah tn abangpengatura...\n",
              "4       sharing alam aja kemarin jam batalin tiket sta...\n",
              "                              ...                        \n",
              "4396    tahu kamu papa mejam mata tahan gejolak batin ...\n",
              "4397    sulit tetap calon wapresnya jokowi pilpres sal...\n",
              "4398    masa depan enggak jelas lah iya bagaimana mau ...\n",
              "4399    dulu benar mahasiswa teknik ui tembak pacar pa...\n",
              "4400       allah engkau tahu rasa sakit hati sembuh allah\n",
              "Name: tweet, Length: 4401, dtype: object"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_emosi['tweet']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## use each representation separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X = df_emosi['tweet']\n",
        "y = df_emosi['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1769    kaget berita tetangga satu rt solo tahun tingg...\n",
              "1220          tidak-enak sangat bada pulang kerjan banyak\n",
              "44      iya ibu nya lahir anak cewek enggak tahu tahun...\n",
              "289     jiyeeee jiyeee jeng dom habis menang lawan kei...\n",
              "2486    cinta penuh banyak buat semua harga tak satu l...\n",
              "                              ...                        \n",
              "3444    sahabat perlu filosopi milik nilai hidup beri ...\n",
              "466     banyak bilang pilih sopir mobil bener bawa mas...\n",
              "3092    bilang tetap pegang janji nikah brhak hakim su...\n",
              "3772    allahapa kok disalahin pres kayak presiden kur...\n",
              "860     gue punya teman dibela-belain pinjem duwit kan...\n",
              "Name: tweet, Length: 3520, dtype: object"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bag of Words\n",
        "vectorizer_bow = CountVectorizer()\n",
        "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
        "X_test_bow = vectorizer_bow.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 4810)\t1\n",
            "  (0, 1147)\t1\n",
            "  (0, 10739)\t1\n",
            "  (0, 9318)\t1\n",
            "  (0, 9102)\t1\n",
            "  (0, 10019)\t1\n",
            "  (0, 10380)\t1\n",
            "  (0, 10859)\t1\n",
            "  (0, 2700)\t1\n",
            "  (0, 11153)\t1\n",
            "  (0, 346)\t1\n",
            "  (0, 349)\t1\n",
            "  (0, 7638)\t1\n",
            "  (0, 1087)\t2\n",
            "  (0, 4210)\t1\n",
            "  (0, 5970)\t1\n",
            "  (0, 11245)\t1\n",
            "  (0, 4273)\t1\n",
            "  (0, 11411)\t1\n",
            "  (0, 4271)\t1\n",
            "  (0, 9066)\t1\n",
            "  (0, 9605)\t1\n",
            "  (0, 253)\t1\n",
            "  (0, 3033)\t1\n",
            "  (1, 10821)\t1\n",
            "  :\t:\n",
            "  (3518, 2483)\t1\n",
            "  (3518, 3202)\t1\n",
            "  (3518, 10910)\t1\n",
            "  (3518, 299)\t1\n",
            "  (3518, 8549)\t1\n",
            "  (3518, 3138)\t1\n",
            "  (3518, 4641)\t1\n",
            "  (3519, 1504)\t1\n",
            "  (3519, 3607)\t1\n",
            "  (3519, 10605)\t1\n",
            "  (3519, 10805)\t1\n",
            "  (3519, 8681)\t1\n",
            "  (3519, 10477)\t1\n",
            "  (3519, 9321)\t1\n",
            "  (3519, 4914)\t1\n",
            "  (3519, 8358)\t2\n",
            "  (3519, 11337)\t1\n",
            "  (3519, 5188)\t1\n",
            "  (3519, 7640)\t1\n",
            "  (3519, 470)\t1\n",
            "  (3519, 7951)\t1\n",
            "  (3519, 3390)\t1\n",
            "  (3519, 2290)\t1\n",
            "  (3519, 1050)\t1\n",
            "  (3519, 2713)\t1\n"
          ]
        }
      ],
      "source": [
        "print(X_train_bow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer_tfidf.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 3033)\t0.22954952181025595\n",
            "  (0, 253)\t0.1820641846385591\n",
            "  (0, 9605)\t0.2500003433823169\n",
            "  (0, 9066)\t0.23803737965372196\n",
            "  (0, 4271)\t0.2500003433823169\n",
            "  (0, 11411)\t0.1721368072098088\n",
            "  (0, 4273)\t0.22954952181025595\n",
            "  (0, 11245)\t0.1672891608802246\n",
            "  (0, 5970)\t0.10814039858178676\n",
            "  (0, 4210)\t0.17432331096844592\n",
            "  (0, 1087)\t0.35588121304178577\n",
            "  (0, 7638)\t0.2130384503539307\n",
            "  (0, 349)\t0.1721368072098088\n",
            "  (0, 346)\t0.2500003433823169\n",
            "  (0, 11153)\t0.2500003433823169\n",
            "  (0, 2700)\t0.14427112963688324\n",
            "  (0, 10859)\t0.13868767712190488\n",
            "  (0, 10380)\t0.1268379737967491\n",
            "  (0, 10019)\t0.1997029461808805\n",
            "  (0, 9102)\t0.20251500621062005\n",
            "  (0, 9318)\t0.12241984116408033\n",
            "  (0, 10739)\t0.18864787866613406\n",
            "  (0, 1147)\t0.18357755732589456\n",
            "  (0, 4810)\t0.17925212460881954\n",
            "  (1, 895)\t0.22239438583203394\n",
            "  :\t:\n",
            "  (3518, 1532)\t0.15131114175447472\n",
            "  (3518, 8554)\t0.2080624342218105\n",
            "  (3518, 5214)\t0.1677000294324275\n",
            "  (3518, 5428)\t0.15604174579959643\n",
            "  (3518, 495)\t0.12332068707344476\n",
            "  (3518, 1499)\t0.20312081639137167\n",
            "  (3518, 5010)\t0.14152874735076804\n",
            "  (3519, 2713)\t0.2621542090294831\n",
            "  (3519, 1050)\t0.2621542090294831\n",
            "  (3519, 2290)\t0.2621542090294831\n",
            "  (3519, 3390)\t0.22816461472660593\n",
            "  (3519, 7951)\t0.24070916267197498\n",
            "  (3519, 470)\t0.21236035339582315\n",
            "  (3519, 7640)\t0.249609661084114\n",
            "  (3519, 5188)\t0.249609661084114\n",
            "  (3519, 11337)\t0.23380539975333123\n",
            "  (3519, 8358)\t0.499219322168228\n",
            "  (3519, 4914)\t0.1718822864638018\n",
            "  (3519, 9321)\t0.187966537597177\n",
            "  (3519, 10477)\t0.23380539975333123\n",
            "  (3519, 8681)\t0.12259425128527934\n",
            "  (3519, 10805)\t0.1978190699569588\n",
            "  (3519, 10605)\t0.12878385180301702\n",
            "  (3519, 3607)\t0.09925356756853812\n",
            "  (3519, 1504)\t0.10047659882865577\n"
          ]
        }
      ],
      "source": [
        "print(X_train_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# N-grams (Unigram and Bigram)\n",
        "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2))\n",
        "X_train_ngram = vectorizer_ngram.fit_transform(X_train)\n",
        "X_test_ngram = vectorizer_ngram.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 27918)\t1\n",
            "  (0, 8289)\t1\n",
            "  (0, 64020)\t1\n",
            "  (0, 54790)\t1\n",
            "  (0, 52951)\t1\n",
            "  (0, 59578)\t1\n",
            "  (0, 61327)\t1\n",
            "  (0, 64775)\t1\n",
            "  (0, 16602)\t1\n",
            "  (0, 66347)\t1\n",
            "  (0, 2330)\t1\n",
            "  (0, 2338)\t1\n",
            "  (0, 45157)\t1\n",
            "  (0, 7900)\t2\n",
            "  (0, 24271)\t1\n",
            "  (0, 35879)\t1\n",
            "  (0, 66768)\t1\n",
            "  (0, 24631)\t1\n",
            "  (0, 67551)\t1\n",
            "  (0, 24627)\t1\n",
            "  (0, 52845)\t1\n",
            "  (0, 57176)\t1\n",
            "  (0, 1787)\t1\n",
            "  (0, 18380)\t1\n",
            "  (0, 27922)\t1\n",
            "  :\t:\n",
            "  (3519, 20950)\t1\n",
            "  (3519, 31517)\t1\n",
            "  (3519, 45166)\t1\n",
            "  (3519, 45167)\t1\n",
            "  (3519, 3030)\t1\n",
            "  (3519, 47287)\t1\n",
            "  (3519, 19734)\t1\n",
            "  (3519, 14867)\t1\n",
            "  (3519, 7467)\t1\n",
            "  (3519, 16691)\t1\n",
            "  (3519, 63181)\t1\n",
            "  (3519, 14868)\t1\n",
            "  (3519, 7468)\t1\n",
            "  (3519, 49755)\t1\n",
            "  (3519, 16692)\t1\n",
            "  (3519, 29570)\t1\n",
            "  (3519, 10370)\t1\n",
            "  (3519, 64344)\t1\n",
            "  (3519, 49757)\t1\n",
            "  (3519, 67272)\t1\n",
            "  (3519, 19735)\t1\n",
            "  (3519, 3034)\t1\n",
            "  (3519, 31519)\t1\n",
            "  (3519, 47290)\t1\n",
            "  (3519, 62219)\t1\n"
          ]
        }
      ],
      "source": [
        "print(X_train_ngram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes with Bag of Words\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.71      0.79      0.74       229\n",
            "        fear       0.72      0.64      0.68       119\n",
            "       happy       0.73      0.62      0.67       214\n",
            "        love       0.74      0.73      0.73       119\n",
            "     sadness       0.53      0.58      0.56       200\n",
            "\n",
            "    accuracy                           0.67       881\n",
            "   macro avg       0.68      0.67      0.68       881\n",
            "weighted avg       0.68      0.67      0.67       881\n",
            "\n",
            "Naive Bayes with TF-IDF\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.61      0.84      0.71       229\n",
            "        fear       0.97      0.29      0.44       119\n",
            "       happy       0.71      0.62      0.66       214\n",
            "        love       0.87      0.40      0.55       119\n",
            "     sadness       0.44      0.65      0.53       200\n",
            "\n",
            "    accuracy                           0.61       881\n",
            "   macro avg       0.72      0.56      0.58       881\n",
            "weighted avg       0.68      0.61      0.60       881\n",
            "\n",
            "Naive Bayes with N-grams\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.66      0.81      0.73       229\n",
            "        fear       0.82      0.63      0.71       119\n",
            "       happy       0.70      0.62      0.66       214\n",
            "        love       0.78      0.68      0.73       119\n",
            "     sadness       0.56      0.60      0.58       200\n",
            "\n",
            "    accuracy                           0.67       881\n",
            "   macro avg       0.70      0.67      0.68       881\n",
            "weighted avg       0.68      0.67      0.67       881\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Naive Bayes\n",
        "nb = MultinomialNB()\n",
        "\n",
        "# Bag of Words\n",
        "nb.fit(X_train_bow, y_train)\n",
        "y_pred_bow_nb = nb.predict(X_test_bow)\n",
        "print(\"Naive Bayes with Bag of Words\")\n",
        "print(classification_report(y_test, y_pred_bow_nb))\n",
        "\n",
        "# TF-IDF\n",
        "nb.fit(X_train_tfidf, y_train)\n",
        "y_pred_tfidf_nb = nb.predict(X_test_tfidf)\n",
        "print(\"Naive Bayes with TF-IDF\")\n",
        "print(classification_report(y_test, y_pred_tfidf_nb))\n",
        "\n",
        "# N-grams\n",
        "nb.fit(X_train_ngram, y_train)\n",
        "y_pred_ngram_nb = nb.predict(X_test_ngram)\n",
        "print(\"Naive Bayes with N-grams\")\n",
        "print(classification_report(y_test, y_pred_ngram_nb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with Bag of Words\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.54      0.76      0.64       229\n",
            "        fear       0.86      0.63      0.73       119\n",
            "       happy       0.71      0.57      0.63       214\n",
            "        love       0.66      0.82      0.73       119\n",
            "     sadness       0.53      0.41      0.46       200\n",
            "\n",
            "    accuracy                           0.62       881\n",
            "   macro avg       0.66      0.64      0.64       881\n",
            "weighted avg       0.64      0.62      0.62       881\n",
            "\n",
            "Random Forest with TF-IDF\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.56      0.76      0.64       229\n",
            "        fear       0.83      0.61      0.71       119\n",
            "       happy       0.72      0.57      0.63       214\n",
            "        love       0.67      0.79      0.72       119\n",
            "     sadness       0.49      0.41      0.45       200\n",
            "\n",
            "    accuracy                           0.62       881\n",
            "   macro avg       0.65      0.63      0.63       881\n",
            "weighted avg       0.63      0.62      0.62       881\n",
            "\n",
            "Random Forest with N-grams\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.53      0.80      0.64       229\n",
            "        fear       0.87      0.60      0.71       119\n",
            "       happy       0.71      0.57      0.63       214\n",
            "        love       0.70      0.79      0.74       119\n",
            "     sadness       0.53      0.39      0.45       200\n",
            "\n",
            "    accuracy                           0.62       881\n",
            "   macro avg       0.67      0.63      0.63       881\n",
            "weighted avg       0.64      0.62      0.62       881\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Random Forest\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Bag of Words\n",
        "rf.fit(X_train_bow, y_train)\n",
        "y_pred_bow_rf = rf.predict(X_test_bow)\n",
        "print(\"Random Forest with Bag of Words\")\n",
        "print(classification_report(y_test, y_pred_bow_rf))\n",
        "\n",
        "# TF-IDF\n",
        "rf.fit(X_train_tfidf, y_train)\n",
        "y_pred_tfidf_rf = rf.predict(X_test_tfidf)\n",
        "print(\"Random Forest with TF-IDF\")\n",
        "print(classification_report(y_test, y_pred_tfidf_rf))\n",
        "\n",
        "# N-grams\n",
        "rf.fit(X_train_ngram, y_train)\n",
        "y_pred_ngram_rf = rf.predict(X_test_ngram)\n",
        "print(\"Random Forest with N-grams\")\n",
        "print(classification_report(y_test, y_pred_ngram_rf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM with Bag of Words\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.58      0.77      0.67       229\n",
            "        fear       0.87      0.51      0.65       119\n",
            "       happy       0.62      0.63      0.62       214\n",
            "        love       0.75      0.72      0.74       119\n",
            "     sadness       0.50      0.43      0.46       200\n",
            "\n",
            "    accuracy                           0.62       881\n",
            "   macro avg       0.66      0.61      0.63       881\n",
            "weighted avg       0.63      0.62      0.62       881\n",
            "\n",
            "SVM with TF-IDF\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.62      0.81      0.70       229\n",
            "        fear       0.88      0.55      0.67       119\n",
            "       happy       0.66      0.65      0.66       214\n",
            "        love       0.79      0.70      0.74       119\n",
            "     sadness       0.53      0.51      0.52       200\n",
            "\n",
            "    accuracy                           0.65       881\n",
            "   macro avg       0.70      0.64      0.66       881\n",
            "weighted avg       0.67      0.65      0.65       881\n",
            "\n",
            "SVM with N-grams\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.59      0.75      0.66       229\n",
            "        fear       0.84      0.49      0.62       119\n",
            "       happy       0.59      0.64      0.61       214\n",
            "        love       0.75      0.76      0.75       119\n",
            "     sadness       0.50      0.42      0.46       200\n",
            "\n",
            "    accuracy                           0.61       881\n",
            "   macro avg       0.65      0.61      0.62       881\n",
            "weighted avg       0.63      0.61      0.61       881\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Support Vector Machine\n",
        "svm = SVC()\n",
        "\n",
        "# Bag of Words\n",
        "svm.fit(X_train_bow, y_train)\n",
        "y_pred_bow_svm = svm.predict(X_test_bow)\n",
        "print(\"SVM with Bag of Words\")\n",
        "print(classification_report(y_test, y_pred_bow_svm))\n",
        "\n",
        "# TF-IDF\n",
        "svm.fit(X_train_tfidf, y_train)\n",
        "y_pred_tfidf_svm = svm.predict(X_test_tfidf)\n",
        "print(\"SVM with TF-IDF\")\n",
        "print(classification_report(y_test, y_pred_tfidf_svm))\n",
        "\n",
        "# N-grams\n",
        "svm.fit(X_train_ngram, y_train)\n",
        "y_pred_ngram_svm = svm.predict(X_test_ngram)\n",
        "print(\"SVM with N-grams\")\n",
        "print(classification_report(y_test, y_pred_ngram_svm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## use three representation in one model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultinomialNB with Combined Features\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.67      0.82      0.73       229\n",
            "        fear       0.80      0.62      0.70       119\n",
            "       happy       0.71      0.62      0.66       214\n",
            "        love       0.77      0.69      0.73       119\n",
            "     sadness       0.53      0.58      0.56       200\n",
            "\n",
            "    accuracy                           0.67       881\n",
            "   macro avg       0.70      0.66      0.68       881\n",
            "weighted avg       0.68      0.67      0.67       881\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define the feature extraction steps\n",
        "vectorizer_bow = CountVectorizer()\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2))\n",
        "\n",
        "# Combine the features using FeatureUnion\n",
        "combined_features = FeatureUnion([\n",
        "    (\"bow\", vectorizer_bow),\n",
        "    (\"tfidf\", vectorizer_tfidf),\n",
        "    (\"ngram\", vectorizer_ngram)\n",
        "])\n",
        "\n",
        "# Create a pipeline that first transforms the data and then applies the model\n",
        "pipeline = Pipeline([\n",
        "    (\"features\", combined_features),\n",
        "    (\"classifier\", MultinomialNB())  # You can replace MultinomialNB with any other classifier\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(\"MultinomialNB with Combined Features\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with Combined Features\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.52      0.79      0.62       229\n",
            "        fear       0.86      0.63      0.73       119\n",
            "       happy       0.69      0.55      0.61       214\n",
            "        love       0.67      0.81      0.73       119\n",
            "     sadness       0.55      0.36      0.43       200\n",
            "\n",
            "    accuracy                           0.61       881\n",
            "   macro avg       0.66      0.63      0.63       881\n",
            "weighted avg       0.63      0.61      0.61       881\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pipeline_rf = Pipeline([\n",
        "    (\"features\", combined_features),\n",
        "    (\"classifier\", RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline_rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_rf = pipeline_rf.predict(X_test)\n",
        "print(\"Random Forest with Combined Features\")\n",
        "print(classification_report(y_test, y_pred_rf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM with Combined Features\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.58      0.75      0.66       229\n",
            "        fear       0.86      0.50      0.63       119\n",
            "       happy       0.60      0.63      0.62       214\n",
            "        love       0.75      0.75      0.75       119\n",
            "     sadness       0.49      0.42      0.45       200\n",
            "\n",
            "    accuracy                           0.61       881\n",
            "   macro avg       0.65      0.61      0.62       881\n",
            "weighted avg       0.62      0.61      0.61       881\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pipeline_svm = Pipeline([\n",
        "    (\"features\", combined_features),\n",
        "    (\"classifier\", SVC())\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline_svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_svm = pipeline_svm.predict(X_test)\n",
        "print(\"SVM with Combined Features\")\n",
        "print(classification_report(y_test, y_pred_svm))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qKN9IBvcoReY",
        "1ip-ys6uoaY0"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
